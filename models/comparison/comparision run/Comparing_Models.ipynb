{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Comparing Models.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIspczK6PQQL",
        "outputId": "41b69d39-62c8-4406-b959-c6ef5836555c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkICI16gO8QF"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "import sklearn.metrics"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "TvmjKYKIO8QI",
        "outputId": "dedbc0b6-be46-4844-d162-a1c5296320e8"
      },
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/College/PS - 1/Notebooks/manuscript/models/comparison/unnormalized.csv')\n",
        "data.drop(columns=['Unnamed: 0'], inplace = True)\n",
        "data"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Age</th>\n",
              "      <th>Gender</th>\n",
              "      <th>BMI</th>\n",
              "      <th>Symptoms Present?</th>\n",
              "      <th>Fever</th>\n",
              "      <th>Cough</th>\n",
              "      <th>Breathlessness</th>\n",
              "      <th>Travel History</th>\n",
              "      <th>Temp</th>\n",
              "      <th>SPO2</th>\n",
              "      <th>Contact to NCOVID Patient</th>\n",
              "      <th>Co-morbidity?</th>\n",
              "      <th>Diabetes</th>\n",
              "      <th>Hypertension</th>\n",
              "      <th>Heart Condition</th>\n",
              "      <th>Respiratory rate(breaths per minute)</th>\n",
              "      <th>Outcome</th>\n",
              "      <th>qSOFA SCORE</th>\n",
              "      <th>HEMOGLOBIN</th>\n",
              "      <th>TLC COUNT</th>\n",
              "      <th>PLATELET COUNT</th>\n",
              "      <th>RANDOM BLOOD SUGAR</th>\n",
              "      <th>UREA</th>\n",
              "      <th>CREATININE</th>\n",
              "      <th>SODIUM</th>\n",
              "      <th>POTASSIUM</th>\n",
              "      <th>CHLORIDE</th>\n",
              "      <th>TOTAL BILIRUBIN</th>\n",
              "      <th>DIRECT BILIRUBIN</th>\n",
              "      <th>SGOT</th>\n",
              "      <th>SGPT</th>\n",
              "      <th>TOTAL PROTEINS</th>\n",
              "      <th>ALBUMIN</th>\n",
              "      <th>ALKALINE PHOSPHATASE</th>\n",
              "      <th>C-REACTIVE PROTEINS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>53</td>\n",
              "      <td>1</td>\n",
              "      <td>22.5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>96.8</td>\n",
              "      <td>99.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.9</td>\n",
              "      <td>6200.0</td>\n",
              "      <td>127000.0</td>\n",
              "      <td>121.0</td>\n",
              "      <td>47.5</td>\n",
              "      <td>1.3</td>\n",
              "      <td>143.0</td>\n",
              "      <td>4.8</td>\n",
              "      <td>108.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>81.3</td>\n",
              "      <td>70.0</td>\n",
              "      <td>5.9</td>\n",
              "      <td>3.8</td>\n",
              "      <td>44.1</td>\n",
              "      <td>58.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "      <td>25.7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>98.7</td>\n",
              "      <td>98.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.4</td>\n",
              "      <td>4140.0</td>\n",
              "      <td>188000.0</td>\n",
              "      <td>96.0</td>\n",
              "      <td>21.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>148.0</td>\n",
              "      <td>4.1</td>\n",
              "      <td>108.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.1</td>\n",
              "      <td>22.2</td>\n",
              "      <td>14.8</td>\n",
              "      <td>6.6</td>\n",
              "      <td>3.9</td>\n",
              "      <td>58.5</td>\n",
              "      <td>3.66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>28</td>\n",
              "      <td>1</td>\n",
              "      <td>22.2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>98.4</td>\n",
              "      <td>98.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.9</td>\n",
              "      <td>4680.0</td>\n",
              "      <td>231000.0</td>\n",
              "      <td>102.0</td>\n",
              "      <td>18.3</td>\n",
              "      <td>5800.0</td>\n",
              "      <td>159.0</td>\n",
              "      <td>18.1</td>\n",
              "      <td>1.1</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.3</td>\n",
              "      <td>19.3</td>\n",
              "      <td>12.8</td>\n",
              "      <td>7.0</td>\n",
              "      <td>4.2</td>\n",
              "      <td>86.0</td>\n",
              "      <td>10.17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>73</td>\n",
              "      <td>1</td>\n",
              "      <td>21.5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>98.0</td>\n",
              "      <td>98.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>13.2</td>\n",
              "      <td>5200.0</td>\n",
              "      <td>234000.0</td>\n",
              "      <td>119.0</td>\n",
              "      <td>36.6</td>\n",
              "      <td>0.9</td>\n",
              "      <td>140.0</td>\n",
              "      <td>4.2</td>\n",
              "      <td>104.0</td>\n",
              "      <td>2.4</td>\n",
              "      <td>1.2</td>\n",
              "      <td>59.0</td>\n",
              "      <td>47.9</td>\n",
              "      <td>6.3</td>\n",
              "      <td>3.7</td>\n",
              "      <td>120.0</td>\n",
              "      <td>168.90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>49</td>\n",
              "      <td>1</td>\n",
              "      <td>27.4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>101.0</td>\n",
              "      <td>98.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.7</td>\n",
              "      <td>9500.0</td>\n",
              "      <td>321000.0</td>\n",
              "      <td>92.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>124.0</td>\n",
              "      <td>3.8</td>\n",
              "      <td>92.0</td>\n",
              "      <td>4.2</td>\n",
              "      <td>2.1</td>\n",
              "      <td>44.6</td>\n",
              "      <td>55.5</td>\n",
              "      <td>5.9</td>\n",
              "      <td>3.1</td>\n",
              "      <td>177.0</td>\n",
              "      <td>164.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>170</th>\n",
              "      <td>53</td>\n",
              "      <td>1</td>\n",
              "      <td>27.2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>98.9</td>\n",
              "      <td>96.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>28.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>18.2</td>\n",
              "      <td>6700.0</td>\n",
              "      <td>101000.0</td>\n",
              "      <td>200.0</td>\n",
              "      <td>54.1</td>\n",
              "      <td>0.9</td>\n",
              "      <td>136.0</td>\n",
              "      <td>2.9</td>\n",
              "      <td>97.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.5</td>\n",
              "      <td>43.8</td>\n",
              "      <td>38.8</td>\n",
              "      <td>6.2</td>\n",
              "      <td>3.7</td>\n",
              "      <td>73.3</td>\n",
              "      <td>127.60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>171</th>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "      <td>26.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>99.3</td>\n",
              "      <td>98.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>28.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>13.9</td>\n",
              "      <td>6800.0</td>\n",
              "      <td>156000.0</td>\n",
              "      <td>151.0</td>\n",
              "      <td>59.6</td>\n",
              "      <td>1.9</td>\n",
              "      <td>141.0</td>\n",
              "      <td>4.2</td>\n",
              "      <td>106.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>80.6</td>\n",
              "      <td>42.6</td>\n",
              "      <td>6.6</td>\n",
              "      <td>3.8</td>\n",
              "      <td>57.4</td>\n",
              "      <td>138.15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>172</th>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "      <td>21.4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>98.3</td>\n",
              "      <td>99.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>14.9</td>\n",
              "      <td>8900.0</td>\n",
              "      <td>1820000.0</td>\n",
              "      <td>189.0</td>\n",
              "      <td>25.2</td>\n",
              "      <td>1.1</td>\n",
              "      <td>137.0</td>\n",
              "      <td>4.2</td>\n",
              "      <td>106.0</td>\n",
              "      <td>1.8</td>\n",
              "      <td>0.6</td>\n",
              "      <td>77.0</td>\n",
              "      <td>27.9</td>\n",
              "      <td>5.9</td>\n",
              "      <td>3.3</td>\n",
              "      <td>60.1</td>\n",
              "      <td>143.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>173</th>\n",
              "      <td>65</td>\n",
              "      <td>0</td>\n",
              "      <td>22.4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>98.9</td>\n",
              "      <td>97.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>11.2</td>\n",
              "      <td>12000.0</td>\n",
              "      <td>124000.0</td>\n",
              "      <td>210.0</td>\n",
              "      <td>43.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>132.0</td>\n",
              "      <td>3.8</td>\n",
              "      <td>110.0</td>\n",
              "      <td>1.2</td>\n",
              "      <td>0.6</td>\n",
              "      <td>56.2</td>\n",
              "      <td>43.2</td>\n",
              "      <td>5.6</td>\n",
              "      <td>3.4</td>\n",
              "      <td>216.0</td>\n",
              "      <td>124.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174</th>\n",
              "      <td>75</td>\n",
              "      <td>1</td>\n",
              "      <td>26.2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>99.0</td>\n",
              "      <td>95.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>28.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>15.3</td>\n",
              "      <td>5500.0</td>\n",
              "      <td>191000.0</td>\n",
              "      <td>149.0</td>\n",
              "      <td>54.6</td>\n",
              "      <td>1.5</td>\n",
              "      <td>127.0</td>\n",
              "      <td>6.7</td>\n",
              "      <td>93.0</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.3</td>\n",
              "      <td>474.2</td>\n",
              "      <td>157.9</td>\n",
              "      <td>6.6</td>\n",
              "      <td>3.3</td>\n",
              "      <td>320.9</td>\n",
              "      <td>163.15</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>175 rows × 35 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Age  Gender   BMI  ...  ALBUMIN  ALKALINE PHOSPHATASE  C-REACTIVE PROTEINS\n",
              "0     53       1  22.5  ...      3.8                  44.1                58.10\n",
              "1     26       0  25.7  ...      3.9                  58.5                 3.66\n",
              "2     28       1  22.2  ...      4.2                  86.0                10.17\n",
              "3     73       1  21.5  ...      3.7                 120.0               168.90\n",
              "4     49       1  27.4  ...      3.1                 177.0               164.00\n",
              "..   ...     ...   ...  ...      ...                   ...                  ...\n",
              "170   53       1  27.2  ...      3.7                  73.3               127.60\n",
              "171   33       1  26.0  ...      3.8                  57.4               138.15\n",
              "172   70       1  21.4  ...      3.3                  60.1               143.00\n",
              "173   65       0  22.4  ...      3.4                 216.0               124.00\n",
              "174   75       1  26.2  ...      3.3                 320.9               163.15\n",
              "\n",
              "[175 rows x 35 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2NL-mJEO8QJ"
      },
      "source": [
        "def algorithm_pipeline(x_train,y_train, model, param_grid, cv = 5, scoring_fit = 'f1'):\n",
        "    \n",
        "    gs = GridSearchCV(\n",
        "        estimator = model,\n",
        "        param_grid = param_grid, \n",
        "        cv = cv, \n",
        "        n_jobs = -1, \n",
        "        scoring = scoring_fit,\n",
        "        verbose = 1\n",
        "    )\n",
        "    fitted_model = gs.fit(x_train, y_train)\n",
        "    \n",
        "    return gs.best_params_"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBAtZ3VZO8QJ"
      },
      "source": [
        "# Splitting the Dataset\n",
        "\n",
        "It is crucial to split the\n",
        "data before performing any further transformations such as scaling the data because we\n",
        "want to prevent any information about the test set to spill over into our training and\n",
        "validation set. Data scaling is often done using statistics about the data set as a whole,\n",
        "such as mean and standard deviation. Because we want to be able to measure how well\n",
        "our Machine Learning models perform on data they have never seen before we have to\n",
        "make sure that no information from the test data impacts how the scaling or any other\n",
        "transformation is done.\n",
        "\n",
        "Before we do any further analysis using our data we need to split the entire data set into\n",
        "three different subsets: training set, validation set and test set.\n",
        "\n",
        "\n",
        "First we make a hold-out testing set (20% of the data). We split the remaining data into a training and a validation set. This allows us to train\n",
        "our models on the training data and then evaluate their performance on the validation\n",
        "data. In theory, we can then go and tweak our models and evaluate them on the\n",
        "validation data again and thereby find ways to improve model performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7x3GyVjO8QK"
      },
      "source": [
        "X = data.drop(['Outcome'],axis=1)\n",
        "Y = data['Outcome']\n",
        "\n",
        "#splitting into train and test set\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.20, random_state = 42)\n",
        "\n",
        "#splitting into train and validation set\n",
        "# x_train, x_valid, y_train, y_valid = train_test_split(x_train_temp, y_train_temp, test_size = 0.25, random_state = 42)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyXfyQtrO8QK"
      },
      "source": [
        "## Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACozLSYPO8QK",
        "outputId": "d0cf532e-6133-4911-8f2a-5fe90062f2b9"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "# Fit and transform using the training data\n",
        "scaler.fit_transform(x_train)\n",
        "# Transform the validation and test features\n",
        "scaler.transform(x_test)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3.49397590e-01, 1.00000000e+00, 6.20689655e-01, ...,\n",
              "        4.44444444e-01, 2.58578856e-01, 9.45129955e-04],\n",
              "       [3.13253012e-01, 0.00000000e+00, 5.28735632e-01, ...,\n",
              "        4.44444444e-01, 1.61178510e-01, 6.74717774e-03],\n",
              "       [2.89156627e-01, 1.00000000e+00, 3.21839080e-01, ...,\n",
              "        7.22222222e-01, 1.13344887e-01, 4.41060646e-03],\n",
              "       ...,\n",
              "       [8.07228916e-01, 0.00000000e+00, 3.21839080e-01, ...,\n",
              "        5.00000000e-01, 2.08318891e-01, 1.05014439e-03],\n",
              "       [6.50602410e-01, 1.00000000e+00, 1.00000000e+00, ...,\n",
              "        4.44444444e-01, 8.80415945e-02, 1.49645576e-03],\n",
              "       [5.90361446e-01, 1.00000000e+00, 3.33333333e-01, ...,\n",
              "        3.33333333e-01, 2.39861352e-01, 7.14098189e-02]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R119K0vDO8QL"
      },
      "source": [
        "## Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "NqazW0TPO8QL",
        "outputId": "25e19eb5-bfc4-456e-f497-39df54f3c7e5"
      },
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "\n",
        "model = ExtraTreesClassifier()\n",
        "model.fit(x_train,y_train)\n",
        "print(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n",
        "#plot graph of feature importances for better visualization\n",
        "feat_importances = pd.Series(model.feature_importances_, index=x_train.columns)\n",
        "feat_importances.nlargest(10).plot(kind='barh')\n",
        "plt.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.04550679 0.01396379 0.02309899 0.00219665 0.00926818 0.01697187\n",
            " 0.06483165 0.00788392 0.01334029 0.02083613 0.01729711 0.01526737\n",
            " 0.01030745 0.01758923 0.01027466 0.13778363 0.04724459 0.01674511\n",
            " 0.04727856 0.03543154 0.01642983 0.03393397 0.02817377 0.01749112\n",
            " 0.02002972 0.03200897 0.02334796 0.02369059 0.03672037 0.01947796\n",
            " 0.03077878 0.01449388 0.02657146 0.10373413]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjU9MInwO8QM"
      },
      "source": [
        "features = ['C-REACTIVE PROTEINS','Respiratory rate(breaths per minute)','Breathlessness','TLC COUNT','Age','SGOT','UREA']"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yp2alEzGO8QM",
        "outputId": "02f56a5a-cc07-42dc-9d37-a6531719ddff"
      },
      "source": [
        "model = RandomForestClassifier()\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [400, 700, 1000],\n",
        "    'max_depth': [15,20,25],\n",
        "    'max_leaf_nodes': [50, 100, 200]\n",
        "}\n",
        "\n",
        "bestparams = algorithm_pipeline(x_train[features], y_train, model, param_grid)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:   32.9s\n",
            "[Parallel(n_jobs=-1)]: Done 135 out of 135 | elapsed:  1.6min finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8fslNmQO8QM",
        "outputId": "89ebe4ad-759a-4dea-e07c-db60cf1b81e1"
      },
      "source": [
        "bestparams"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'max_depth': 20, 'max_leaf_nodes': 50, 'n_estimators': 1000}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRoT5kRLO8QM",
        "outputId": "3aeb164d-e633-4fdb-a340-f3e1971036ce"
      },
      "source": [
        "model = LogisticRegression(max_iter = 8000)\n",
        "param_grid = {'solver' : ['newton-cg', 'lbfgs', 'liblinear','sag','saga'],\n",
        "'penalty' : ['l2'],\n",
        "'C' : [300, 100, 30, 10, 3, 1.0, 0.3, 0.1, 0.03, 0.01]} \n",
        "\n",
        "bestparams = algorithm_pipeline(x_train[features], y_train, model, param_grid)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  88 tasks      | elapsed:    4.8s\n",
            "[Parallel(n_jobs=-1)]: Done 250 out of 250 | elapsed:   13.9s finished\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/linesearch.py:466: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/linesearch.py:466: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/linesearch.py:466: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6jZQWujO8QN",
        "outputId": "38ec2706-f216-4651-e1f1-d95c16e3bbef"
      },
      "source": [
        "bestparams"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'C': 3, 'penalty': 'l2', 'solver': 'newton-cg'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4F617p9O8QO",
        "outputId": "95b873e1-c935-41d1-bcd0-872c43dd7713"
      },
      "source": [
        "mlp = MLPClassifier(max_iter=8000)\n",
        "param_grid = {\n",
        "    'hidden_layer_sizes': [(50,), (100,), (50,100)],\n",
        "    'activation': ['tanh', 'relu','logistic'],\n",
        "    'solver': ['lbfgs','sgd'],\n",
        "    'alpha': [0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3],\n",
        "    'learning_rate': ['constant','adaptive','invscaling'],\n",
        "}\n",
        "bestparams = algorithm_pipeline(x_train[features], y_train, mlp, param_grid)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done 188 tasks      | elapsed:   11.5s\n",
            "[Parallel(n_jobs=-1)]: Done 492 tasks      | elapsed:   54.0s\n",
            "[Parallel(n_jobs=-1)]: Done 747 tasks      | elapsed:  4.5min\n",
            "[Parallel(n_jobs=-1)]: Done 1891 tasks      | elapsed:  5.5min\n",
            "[Parallel(n_jobs=-1)]: Done 2160 out of 2160 | elapsed: 12.2min finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-lkPcxbO8QP",
        "outputId": "a755ad9d-402f-4463-8a2f-416ea77fa4e4"
      },
      "source": [
        "bestparams"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'activation': 'logistic',\n",
              " 'alpha': 0.3,\n",
              " 'hidden_layer_sizes': (50,),\n",
              " 'learning_rate': 'constant',\n",
              " 'solver': 'lbfgs'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "LFsdoHcmO8QP",
        "outputId": "15928ee7-301d-4501-de32-eea485783b17"
      },
      "source": [
        "svm = SVC()\n",
        "param_grid = {'C': [0.1, 0.3, 1, 3, 10, 30, 100, 300, 1000], \n",
        "              'gamma': [3, 1, 0.3, 0.1, 0.03, 0.01, 0.003, 0.001, 0.0003, 0.0001, 'scale','auto'],\n",
        "              'kernel': ['rbf','linear','sigmoid','poly']} \n",
        "\n",
        "bestparams = algorithm_pipeline(x_train[features], y_train, svm, param_grid)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-5d292a89c978>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m               'kernel': ['rbf','linear','sigmoid','poly']} \n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mbestparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malgorithm_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-4e6b5de6c0a8>\u001b[0m in \u001b[0;36malgorithm_pipeline\u001b[0;34m(x_train, y_train, model, param_grid, cv, scoring_fit)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     )\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mfitted_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    687\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 689\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    931\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaHMC-h0O8QP"
      },
      "source": [
        "bestparams"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hB6VO555O8QP",
        "outputId": "2cf46978-9a1c-4d5e-84d2-94a92e21beef"
      },
      "source": [
        "xgb = XGBClassifier()\n",
        "param_grid = {\n",
        "    'n_estimators': [400, 700, 1000],\n",
        "    'colsample_bytree': [0.7, 0.8],\n",
        "    'max_depth': [15,20,25],\n",
        "    'reg_alpha': [1.1, 1.2, 1.3],\n",
        "    'reg_lambda': [1.1, 1.2, 1.3],\n",
        "    'subsample': [0.7, 0.8, 0.9]\n",
        "}\n",
        "bestparams = algorithm_pipeline(x_train[features], y_train, xgb, param_grid)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 486 candidates, totalling 2430 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:    5.1s\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:   17.0s\n",
            "[Parallel(n_jobs=-1)]: Done 446 tasks      | elapsed:   46.7s\n",
            "[Parallel(n_jobs=-1)]: Done 796 tasks      | elapsed:  1.4min\n",
            "[Parallel(n_jobs=-1)]: Done 1246 tasks      | elapsed:  2.2min\n",
            "[Parallel(n_jobs=-1)]: Done 1796 tasks      | elapsed:  3.0min\n",
            "[Parallel(n_jobs=-1)]: Done 2430 out of 2430 | elapsed:  4.2min finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVG04EJ7O8QQ"
      },
      "source": [
        "Now define all the models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "407yaR5GO8QQ"
      },
      "source": [
        "rf = RandomForestClassifier(max_depth = 25, max_leaf_nodes = 50, n_estimators = 400)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KD1RL07UO8QQ"
      },
      "source": [
        "# Define a function that compares the CV perfromance of a set of predetrmined models \n",
        "def cv_comparison(models, X, y, cv):\n",
        "    \n",
        "    # Initiate a DataFrame for the averages and a list for all measures\n",
        "    cv_accuracies = pd.DataFrame()\n",
        "    maes = []\n",
        "    mses = []\n",
        "    r2s = []\n",
        "    accs = []\n",
        "    \n",
        "    # Loop through the models, run a CV, add the average scores to the DataFrame and the scores of \n",
        "    # all CVs to the list\n",
        "    \n",
        "    for model in models:\n",
        "        mae = -np.round(cross_val_score(model, X, y, scoring='f1', cv=cv), 4)\n",
        "        maes.append(mae)\n",
        "        mae_avg = round(mae.mean(), 4)\n",
        "        mse = -np.round(cross_val_score(model, X, y, scoring='roc_auc', cv=cv), 4)\n",
        "        mses.append(mse)\n",
        "        mse_avg = round(mse.mean(), 4)\n",
        "        r2 = np.round(cross_val_score(model, X, y, scoring='r2', cv=cv), 4)\n",
        "        r2s.append(r2)\n",
        "        r2_avg = round(r2.mean(), 4)\n",
        "        acc = np.round((100 - (100 * (mae * len(X))) / sum(y)), 4)\n",
        "        accs.append(acc)\n",
        "        acc_avg = round(acc.mean(), 4)\n",
        "        cv_accuracies[str(model)] = [mae_avg, mse_avg, r2_avg, acc_avg]\n",
        "        \n",
        "    cv_accuracies.index = ['F1 Score', 'ROC-AUC', 'R^2', 'Accuracy']\n",
        "    return cv_accuracies, maes, mses, r2s, accs"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOyXuKVQcWCG"
      },
      "source": [
        "bkjkjkjb"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}